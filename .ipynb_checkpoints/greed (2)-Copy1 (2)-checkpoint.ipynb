{
 "cells": [
  {
   "cell_type": "raw",
   "id": "f9b66daa-651f-472f-895d-7eb200d6264d",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- Knee method to get the thresholds in paragraph partitioning\n",
    "- Interface to showcase the work\n",
    "\n",
    "FUTURE WORKS:\n",
    "- Compile dataset\n",
    "- Text smoothing to increase coherence\n",
    "- Include Tables and Diagrams into the summary\n",
    "- Option to select summarization algorithms (TextRank, SumBase etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1424066b-e01b-46a4-af65-317f85195771",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from itertools import chain\n",
    "from sklearn.metrics import pairwise_distances\n",
    "#from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import fasttext as ft\n",
    "from collections import defaultdict\n",
    "#from greed import greed_sum_query\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "\n",
    "import pickle\n",
    "def save_obj(obj, name):\n",
    "    pickle.dump(obj,open(name + '.pkl', 'wb'), protocol=4)\n",
    "    \n",
    "def load_obj(name):\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "sw = stopwords.words('russian')\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b33c900-86d3-48bd-b41d-adb598726f0b",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627b3c9b-0c06-4a6b-998f-9f1bed552a54",
   "metadata": {},
   "source": [
    "## Get file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8558fb-cc09-446c-ba37-56984483ef0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get file paths in all data folders and subfolders\n",
    "# https://pynative.com/python-list-files-in-a-directory/#:~:text=You%20can%20use%20os.,current_path%2C%20files%20in%20current_path).\n",
    "import glob\n",
    "\n",
    "file_names = []\n",
    "# get all files inside a specific folder\n",
    "dir_path = './data/**/*.*'\n",
    "for file in glob.glob(dir_path, recursive=True):\n",
    "    print(file)\n",
    "    file_names.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c350a4-108f-4b42-9a5e-84cee8f24052",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ac4955-9605-4479-aca5-2c168bda8534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out doc files\n",
    "doc_files = [f for f in file_names if '.doc'in f]\n",
    "len(doc_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d275f2e-db27-4ba2-8c6f-8daaa9eed2dc",
   "metadata": {},
   "source": [
    "## Get texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837de5f5-7bbe-434a-b965-a30a5f6b3c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx2txt\n",
    "\n",
    "# extract text\n",
    "text = docx2txt.process(doc_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714e8b0b-53c4-4b57-90e2-8a5677eacfd4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text = ' '.join(text.split('\\n\\n'))\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4654c99-ddb6-471e-a1de-539203513285",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "         \n",
    "for f in tqdm(doc_files):\n",
    "    try:\n",
    "       texts.append(' '.join(docx2txt.process(f).split('\\n\\n')) ) \n",
    "    except:\n",
    "        texts.append('Error')\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebab106-d753-4d2b-952d-9b11c7b78fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8439b5-334c-4813-a1f3-740ed8ba3ec1",
   "metadata": {},
   "source": [
    "# Generate vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075d6f6f-5b71-47d3-9cb9-300162c4301b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext as ft\n",
    "modelPath = \"../com_lang_models/\"\n",
    "model = ft.load_model(modelPath+\"cc.ru.300.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867bb306-fa18-42b5-a12d-069c201ba052",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fasttext = [model.get_sentence_vector(t.replace('\\n','')) for t in tqdm(texts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6ab864-58bb-4ffb-9576-756e0cfab722",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_fasttext[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a827b4c-fc9d-461c-9b39-a329f91f8c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df['title'] = [os.path.basename(p) for p in doc_files]\n",
    "df['text'] = texts\n",
    "df['vecs'] = X_fasttext\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ead1e86-c92d-4a3c-9ae8-add10f7924b4",
   "metadata": {},
   "source": [
    "# GreedSum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43eabc0e-1e6a-4de8-86a3-fe78e403a153",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greed_sum_query(text, query, num_sent=10, min_df=1, max_df=1.0, stop_words=sw):\n",
    "    # Let's take 10% of the most meaningful sentences\n",
    "    #num_sent = 10 # int(len(text)*0.05) \n",
    "    #print('Number of 5% sentences', num_sent)\n",
    "        \n",
    "    #fit a TFIDF vectorizer\n",
    "    #print(min_df, max_df)\n",
    "    vectorizer = TfidfVectorizer(min_df=min_df, max_df=max_df, stop_words=stop_words)\n",
    "    X1 = vectorizer.fit_transform(text).toarray()\n",
    "    \n",
    "    # query specific TFIDF\n",
    "    voc = word_tokenize(query.lower())\n",
    "    vectorizer2 = TfidfVectorizer(vocabulary=voc)\n",
    "    X2 = vectorizer2.fit_transform(text).toarray()\n",
    "    \n",
    "    X = np.concatenate((X1, X2), axis=1)\n",
    "    \n",
    "    # uprank the sentences which are semantically closer to the query and downrank others\n",
    "    query_vec = model.get_sentence_vector(query)\n",
    "    text_vecs = np.array([model.get_sentence_vector(s.replace('\\n','')) for s in text])\n",
    "    similarity = 1 - pairwise_distances(query_vec.reshape(1, -1), text_vecs, metric='cosine')\n",
    "    X = X * similarity[0][:, None]\n",
    "    \n",
    "    #print(X.shape)\n",
    "    #get the sentence indices\n",
    "    idx = []\n",
    "    while sum(sum(X)) != 0:\n",
    "        ind = np.argmax(X.sum(axis=1))\n",
    "        #print(ind)\n",
    "        idx.append(ind)\n",
    "        #print(num_sent, idx)\n",
    "        #stop if we have more than 20% of the sentences from the text\n",
    "        if len(idx) > int(len(text)*0.2):#num_sent:\n",
    "            break\n",
    "            \n",
    "        #update the matrix deleting the columns corresponding to the words found in previous step\n",
    "        cols = X[ind]\n",
    "        col_idx = [i for i in range(len(cols)) if cols[i] > 0]\n",
    "        X = np.delete(X, col_idx, 1)\n",
    "        #print(X.shape)\n",
    "        \n",
    "    #print (len(text), len(idx))\n",
    "    #make a condition to extract a number of sentences or all salient sentences\n",
    "    if num_sent != 0:\n",
    "        idx = idx[:num_sent]\n",
    "    idx.sort()\n",
    "    #print(idx)\n",
    "    summary = [text[i] for i in idx]\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4e5c8f-0c7e-4714-8cee-357133ca3c49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d8f6d5-4ccd-4204-b291-8ce12bee6a3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b1d37e-f5a1-48ff-bc08-68ca3a4c3902",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c98eca-cebe-47a2-9c37-589064895cd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84dccc00-54df-4ea8-a4ba-b03f33956875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters setup\n",
    "top_n = 5\n",
    "num_sent = 0\n",
    "\n",
    "uq = 'Организация процесса хвостового хозяйства'\n",
    "\n",
    "# 1. Select top N documents closest to the query\n",
    "def get_docs(query, vecs, top_n=top_n):\n",
    "    query_vec = model.get_sentence_vector(query.replace('\\n',''))\n",
    "    dists = pairwise_distances(query_vec.reshape(1, -1), vecs, metric='cosine')\n",
    "    idx = np.argsort(dists)[0][: top_n]\n",
    "    #print(df.title.values[idx])\n",
    "    return idx\n",
    "\n",
    "idx = get_docs(uq, np.stack(df.vecs.values))\n",
    "\n",
    "    \n",
    "# 2. Summarize each document regarding query\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from itertools import chain\n",
    "text_sents = sent_tokenize(df.text.values[idx[0]])\n",
    "#text_sents\n",
    "sums = [greed_sum_query(sent_tokenize(t), uq, num_sent=num_sent) for t in df.text.values[idx]]\n",
    "\n",
    "\n",
    "# 3. Merge summaries and summarize them again\n",
    "sums = list(chain(*sums))\n",
    "#len(sums)\n",
    "final_sum = greed_sum_query(sums, uq, num_sent=num_sent)\n",
    "len(final_sum)\n",
    "\n",
    "# 4. Get references for each sentence in the resulting summary\n",
    "refs = []\n",
    "for s in final_sum:\n",
    "    for i, d in enumerate(df.text.values[idx]):\n",
    "        if s in d:\n",
    "            refs.append(i)\n",
    "            break\n",
    "#refs\n",
    "final_sum = [s + '[%s]'%(r+1) for s, r in zip(final_sum, refs)]\n",
    "#print(final_sum)\n",
    "\n",
    "ref_list = ['%s. '%(r+1)+t for r, t in enumerate(df.title.values[idx])]\n",
    "#print(ref_list)\n",
    "\n",
    "\n",
    "# 5. Separate the text into paragraphs\n",
    "## Find paragraph boundaries\n",
    "\n",
    "### consecutive sentence pair similarity\n",
    "sum_sent_vecs = [model.get_sentence_vector(s.replace('\\n','')) for s in final_sum]\n",
    "sims = []\n",
    "for i in range(1, len(sum_sent_vecs)):\n",
    "    sims.append(1 - pairwise_distances(sum_sent_vecs[i].reshape(1, -1), sum_sent_vecs[i-1].reshape(1, -1), metric='cosine')[0][0])\n",
    "\n",
    "treshld = 0.5\n",
    "par_begin = np.array(sims) <= treshld\n",
    "par_begin_idx = np.array(range(len(sims)))[par_begin]\n",
    "#par_begin_idx\n",
    "\n",
    "\n",
    "## Make titles for paragraphs\n",
    "\n",
    "def get_title(par):\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(3, 3),stop_words=sw)\n",
    "    X = vectorizer.fit_transform(par).toarray().sum(axis=0)\n",
    "    return vectorizer.get_feature_names_out()[np.argmax(X)].upper()\n",
    "\n",
    "# par = final_sum[0:par_begin_idx[0]]\n",
    "# get_title(par)\n",
    "\n",
    "# divide a list into parts\n",
    "def partition(alist, indices):\n",
    "    return [alist[i:j] for i, j in zip([0]+indices, indices+[None])]\n",
    "\n",
    "pars = partition(final_sum, list(par_begin_idx))\n",
    "\n",
    "par_tits = [get_title(p) for p in pars]\n",
    "#par_tits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987a388b-1804-41b2-b8b2-d15eea97a229",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 6. Put together text with inline references and a reference list\n",
    "\n",
    "print(('Саммари по запросу: ' + uq).upper())\n",
    "print()\n",
    "\n",
    "for t,p in zip(par_tits, pars):\n",
    "    print(t)\n",
    "    print('. '.join(p))\n",
    "    print()\n",
    "\n",
    "print('СПИСОК ИСПОЛЬЗОВАННОЙ ЛИТЕРАТУРЫ:')\n",
    "for r in ref_list:\n",
    "    print(r)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1a538c-332b-4040-bc5e-dc1a16c32ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "par_begin_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1317d166-1228-4c58-ba8e-6a6ce7764f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a909e57-c83a-4ef3-b7be-df8385c301ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844e9239-8a1a-45bb-8cae-26aa43dc9407",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std(sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea0da92-fd13-421d-9396-1ce843b8e694",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab375bd3-b599-43aa-b2e0-333105e8dea3",
   "metadata": {},
   "source": [
    "# Wikipedia mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6368f4b2-3b4e-453f-bf5a-11bc72aa4991",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059a5d4d-2e01-42de-a3a4-20829b403233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUERY \n",
    "uq = 'Организация процесса добычи твердых полезных ископаемых'\n",
    "\n",
    "# Hyperparameters\n",
    "top_n = 10\n",
    "num_sent = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607349a0-b132-490d-b1c9-0c799a1cf4ee",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Get the most relevant docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8aadd7-d062-45a0-ad97-c50d3a4a941a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wikipedia.set_lang(\"ru\")\n",
    "wiki_titles = wikipedia.search(uq, results=10000, suggestion=False)\n",
    "wiki_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e4942e-5857-4961-b31d-7f26883faaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(wiki_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c64797-b332-406d-93ee-a68a648653f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page(title):\n",
    "    try:\n",
    "        p = wikipedia.page(title, auto_suggest=False, redirect=True, preload=False)\n",
    "        return p\n",
    "    except wikipedia.DisambiguationError as e:\n",
    "        s = e.options[0] #random.choice(e.options)\n",
    "        p = wikipedia.page(s, auto_suggest=False, redirect=True, preload=False)\n",
    "        return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c929ea-e88e-4912-b74d-30826b202d1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "summaries = [get_page(title).summary for title in tqdm(wiki_titles)]\n",
    "summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fb9aa4-7915-4cb8-a758-078530389c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "# select model from https://www.sbert.net/docs/pretrained_models.html\n",
    "model_emb = SentenceTransformer('distiluse-base-multilingual-cased-v1')\n",
    "\n",
    "def similarity_measure(text1, text2):\n",
    "    # based on https://towardsdatascience.com/bert-for-measuring-text-similarity-eec91c6bf9e1\n",
    "    text_embeddings = model_emb.encode([text1, text2])\n",
    "    sim_score = cosine_similarity(text_embeddings[0].reshape(1, -1), text_embeddings[1].reshape(1, -1))\n",
    "    \n",
    "    return sim_score[0][0]\n",
    "\n",
    "sum_vecs = [model_emb.encode(t) for t in tqdm(summaries)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3ab492-93eb-42fc-a69e-1456e903dff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "uq_vec = model_emb.encode(uq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5635f51c-4ac1-4e0d-a687-d4535091af21",
   "metadata": {},
   "outputs": [],
   "source": [
    "uq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff5e220-514b-4cd1-aea5-bfb87fa51fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances, pairwise_distances_chunked \n",
    "dists = pairwise_distances(uq_vec.reshape(1, -1), sum_vecs, metric=\"cosine\")\n",
    "dists.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93775cfe-5368-43df-8a91-f142d0d56d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.argsort(dists[0])[:top_n]\n",
    "dists[0][idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c874345c-ab23-4107-a30f-b76008f45252",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(wiki_titles)[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805cf21c-744e-4469-8203-9a4fc92b2bee",
   "metadata": {},
   "source": [
    "## Load collected wiki articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681d9274-0047-45ab-97cd-bbd61ce32b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_obj('ds_mining_wiki')\n",
    "ds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df664dd2-c487-4487-b7d0-04e9d3106f76",
   "metadata": {},
   "source": [
    "## Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cfe3b4-732a-4ae3-909b-630f461df657",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "# select model from https://www.sbert.net/docs/pretrained_models.html\n",
    "model_emb = SentenceTransformer('distiluse-base-multilingual-cased-v1', device='cuda')\n",
    "\n",
    "def similarity_measure(text1, text2):\n",
    "    # based on https://towardsdatascience.com/bert-for-measuring-text-similarity-eec91c6bf9e1\n",
    "    text_emb1 = model_emb.encode(text1)#model.get_sentence_vector(text1.replace('\\n','').lower())#model_emb.encode([text1, text2])\n",
    "    text_emb2 = model_emb.encode(text2)#model.get_sentence_vector(text2.replace('\\n','').lower())\n",
    "    sim_score = cosine_similarity(text_emb1.reshape(1, -1), text_emb2.reshape(1, -1))\n",
    "    \n",
    "    return sim_score[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4186fd2a-315c-4f97-a5bd-807a91136e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_measure('Организация хвостового хозяйства', 'Хвостохранилище на ГМК')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0db2bef-778f-4049-89a9-b5cc73a3f665",
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_vecs = [model.get_sentence_vector(c.replace('\\n','').lower()) for c in tqdm(ds.title.values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d52b1b-d6a5-4a66-be20-5dad8e0e4932",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['cont_vecs'] = cont_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988b6637-d2b9-4781-8def-02e7665e6517",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_obj(ds, 'ds_mining_wiki_vecs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2744bbb-9144-42aa-b162-3b05c93a0451",
   "metadata": {},
   "source": [
    "## GreedSum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bacb2eb-31e9-4b4d-a432-80d34eeb3791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def greed_sum_query(text, query, num_sent=10, min_df=1, max_df=1.0, stop_words=sw):\n",
    "#     # Let's take 10% of the most meaningful sentences\n",
    "#     #num_sent = 10 # int(len(text)*0.05) \n",
    "#     #print('Number of 5% sentences', num_sent)\n",
    "        \n",
    "#     #fit a TFIDF vectorizer\n",
    "#     #print(min_df, max_df)\n",
    "#     vectorizer = TfidfVectorizer(min_df=min_df, max_df=max_df, stop_words=stop_words)\n",
    "#     X1 = vectorizer.fit_transform(text).toarray()\n",
    "    \n",
    "#     # query specific TFIDF\n",
    "#     voc = word_tokenize(query.lower())\n",
    "#     vectorizer2 = TfidfVectorizer(vocabulary=voc)\n",
    "#     X2 = vectorizer2.fit_transform(text).toarray()\n",
    "    \n",
    "#     X = np.concatenate((X1, X2), axis=1)\n",
    "    \n",
    "#     # uprank the sentences which are semantically closer to the query and downrank others\n",
    "#     query_vec = model_emb.encode(query)\n",
    "#     text_vecs = np.array([model_emb.encode(s.replace('\\n','')) for s in text])\n",
    "#     similarity = 1 - pairwise_distances(query_vec.reshape(1, -1), text_vecs, metric='cosine')\n",
    "#     X = X * similarity[0][:, None]\n",
    "    \n",
    "#     #print(X.shape)\n",
    "#     #get the sentence indices\n",
    "#     idx = []\n",
    "#     while sum(sum(X)) != 0:\n",
    "#         ind = np.argmax(X.sum(axis=1))\n",
    "#         #print(ind)\n",
    "#         idx.append(ind)\n",
    "#         #print(num_sent, idx)\n",
    "#         #stop if we have more than 20% of the sentences from the text\n",
    "#         if len(idx) > int(len(text)*0.2):#num_sent:\n",
    "#             break\n",
    "            \n",
    "#         #update the matrix deleting the columns corresponding to the words found in previous step\n",
    "#         cols = X[ind]\n",
    "#         col_idx = [i for i in range(len(cols)) if cols[i] > 0]\n",
    "#         X = np.delete(X, col_idx, 1)\n",
    "#         #print(X.shape)\n",
    "        \n",
    "#     #print (len(text), len(idx))\n",
    "#     #make a condition to extract a number of sentences or all salient sentences\n",
    "#     if num_sent != 0:\n",
    "#         idx = idx[:num_sent]\n",
    "#     idx.sort()\n",
    "#     #print(idx)\n",
    "#     summary = [text[i] for i in idx]\n",
    "#     return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22730c0e-a6c2-4af4-abe0-cc1d6924b1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters setup\n",
    "top_n = 5\n",
    "num_sent = 0\n",
    "\n",
    "uq = 'Организация процесса хвостового хозяйства'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc1b469-d084-4915-b11a-9baad67579e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Select top N documents closest to the query\n",
    "idx = get_docs(uq, np.stack(ds.cont_vecs.values))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fe2561-1ce1-412d-8f0b-b7e27c124d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae45e143-8667-4cf4-8099-dbf1f1d93028",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.title.values[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ed42be-5107-4b4d-b1b3-10f625c1907d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Summarize each document regarding query\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from itertools import chain\n",
    "#text_sents = sent_tokenize(ds.content.values[idx[0]])\n",
    "#text_sents\n",
    "sums = [greed_sum_query(sent_tokenize(t), uq, num_sent=num_sent) for t in tqdm(ds.content.values[idx])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6f7bf0-622f-4e85-956f-e40c8e1e97ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b924b0a-b1b4-4d68-86c6-7c51fda10208",
   "metadata": {},
   "outputs": [],
   "source": [
    "sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694488df-9941-4226-831e-ba7ad8924efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Merge summaries and summarize them again\n",
    "sums = list(chain(*sums))\n",
    "#len(sums)\n",
    "final_sum = greed_sum_query(sums, uq, num_sent=num_sent)\n",
    "len(final_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d64a6aa-b853-4755-8cf2-acc37bc9ef96",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0cb726-d8f6-484d-85f7-0f3f95a67a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Get references for each sentence in the resulting summary\n",
    "refs = []\n",
    "for s in final_sum:\n",
    "    for i, d in enumerate(ds.content.values[idx]):\n",
    "        if s in d:\n",
    "            refs.append(i)\n",
    "            break\n",
    "#refs\n",
    "final_sum = [s + '[%s]'%(r+1) for s, r in zip(final_sum, refs)]\n",
    "#print(final_sum)\n",
    "\n",
    "ref_list = ['%s. '%(r+1)+t for r, t in enumerate(ds.title.values[idx])]\n",
    "#print(ref_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853b711a-7d87-46a2-b201-6af2b91133bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Separate the text into paragraphs\n",
    "## Find paragraph boundaries\n",
    "\n",
    "### consecutive sentence pair similarity\n",
    "sum_sent_vecs = [model.get_sentence_vector(s.replace('\\n','')) for s in final_sum]\n",
    "sims = []\n",
    "for i in range(1, len(sum_sent_vecs)):\n",
    "    sims.append(1 - pairwise_distances(sum_sent_vecs[i].reshape(1, -1), sum_sent_vecs[i-1].reshape(1, -1), metric='cosine')[0][0])\n",
    "\n",
    "treshld = 0.5\n",
    "par_begin = np.array(sims) <= treshld\n",
    "par_begin_idx = np.array(range(len(sims)))[par_begin]\n",
    "#par_begin_idx\n",
    "\n",
    "\n",
    "## Make titles for paragraphs\n",
    "\n",
    "def get_title(par):\n",
    "    if len(par) > 0:\n",
    "        vectorizer = TfidfVectorizer(ngram_range=(3, 3),stop_words=sw)\n",
    "        X = vectorizer.fit_transform(par).toarray().sum(axis=0)\n",
    "        return vectorizer.get_feature_names_out()[np.argmax(X)].upper()\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "# par = final_sum[0:par_begin_idx[0]]\n",
    "# get_title(par)\n",
    "\n",
    "# divide a list into parts\n",
    "def partition(alist, indices):\n",
    "    return [alist[i:j] for i, j in zip([0]+indices, indices+[None])]\n",
    "\n",
    "pars = partition(final_sum, list(par_begin_idx))\n",
    "\n",
    "par_tits = [get_title(p) for p in pars]\n",
    "#par_tits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a59653-17f6-415f-b3fd-e4f213542202",
   "metadata": {},
   "outputs": [],
   "source": [
    "pars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d4901b-fb8b-44e9-a66a-8478c0e63e42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 6. Put together text with inline references and a reference list\n",
    "\n",
    "print(('Саммари по запросу: ' + uq).upper())\n",
    "print()\n",
    "\n",
    "for t,p in zip(par_tits, pars):\n",
    "    print(t)\n",
    "    print('. '.join(p))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edb3a24-f851-46e4-a5e1-cda07f2dd1fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('СПИСОК ИСПОЛЬЗОВАННОЙ ЛИТЕРАТУРЫ:')\n",
    "for r in ref_list:\n",
    "    print(r)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e045dc8d-f358-41f7-884c-dc26385a565d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
