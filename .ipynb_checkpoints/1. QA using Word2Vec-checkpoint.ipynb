{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "1itJPKIYmnCb"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import unicodedata\n",
    "import nltk\n",
    "import gensim\n",
    "import math\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "gMhTGstymnCb"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './NLP-QuestionAnswerSystem/dataset/text_data/S08_set3_a4.txt.clean'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-e979e685e4dc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ArticleFile'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ArticleText'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ArticleFile'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mgetArticleText\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ArticleText'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ArticleText'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'(\\n)+'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'. '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'DifficultyFromQuestioner'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'DifficultyFromAnswerer'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ArticleFile'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'columns'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlpenv\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   4211\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4212\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4213\u001b[1;33m                 \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4215\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-e979e685e4dc>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ArticleFile'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ArticleText'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ArticleFile'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mgetArticleText\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ArticleText'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ArticleText'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'(\\n)+'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'. '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'DifficultyFromQuestioner'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'DifficultyFromAnswerer'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ArticleFile'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'columns'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-e979e685e4dc>\u001b[0m in \u001b[0;36mgetArticleText\u001b[1;34m(file)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mfpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'./NLP-QuestionAnswerSystem/dataset/text_data/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.txt.clean'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mUnicodeDecodeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './NLP-QuestionAnswerSystem/dataset/text_data/S08_set3_a4.txt.clean'"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv('./S08_question_answer_pairs.txt', sep='\\t')\n",
    "df2 = pd.read_csv('./S09_question_answer_pairs.txt', sep='\\t')\n",
    "df3 = pd.read_csv('./S10_question_answer_pairs.txt', sep='\\t', encoding = 'ISO-8859-1')\n",
    "frames = [df1, df2, df3]\n",
    "df = pd.concat(frames)\n",
    "\n",
    "def getArticleText(file):\n",
    "    fpath = './NLP-QuestionAnswerSystem/dataset/text_data/'+file+'.txt.clean'\n",
    "    try:\n",
    "        f = open(fpath, 'r')\n",
    "        text = f.read()\n",
    "    except UnicodeDecodeError:\n",
    "        f = open(fpath, 'r', encoding = 'ISO-8859-1')\n",
    "        text = f.read()\n",
    "    return text\n",
    "\n",
    "df = df.dropna(subset=['ArticleFile'])\n",
    "df['ArticleText'] = df['ArticleFile'].apply(lambda x: getArticleText(x))\n",
    "df['ArticleText'] = df['ArticleText'].apply(lambda x: re.sub(r'(\\n)+', '. ', x))\n",
    "df = df.drop(['DifficultyFromQuestioner', 'DifficultyFromAnswerer', 'ArticleFile'], axis='columns')\n",
    "\n",
    "def cleanQuestion(text):\n",
    "    text = str(text)\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    text = text.lower()\n",
    "    words = re.sub(r'[^\\w\\s]', '', text).split()\n",
    "    return \" \".join([word for word in words])\n",
    "\n",
    "def cleanAnswer(text):\n",
    "    text = str(text)\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    text = text.lower()\n",
    "    words = re.sub(r'[^\\w\\s]', '', text).split()\n",
    "    return \" \".join([word for word in words])\n",
    "\n",
    "def cleanText(text):\n",
    "    text = str(text)\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    text = text.lower()\n",
    "    words = re.sub(r'[^\\w\\s\\.\\?]', '', text).split()\n",
    "    return \" \".join([word for word in words])\n",
    "\n",
    "df['Question'] = df['Question'].apply(lambda x: cleanQuestion(x))\n",
    "df['Answer'] = df['Answer'].apply(lambda x: cleanAnswer(x))\n",
    "df['ArticleText'] = df['ArticleText'].apply(lambda x: cleanText(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CtYR4nVPmnCb"
   },
   "outputs": [],
   "source": [
    "dataset = []\n",
    "title = \"\"\n",
    "for i in range(0, len(df), 2):\n",
    "    this_title = df.iloc[i]['ArticleTitle']\n",
    "    if (this_title!=title):\n",
    "        title = this_title\n",
    "        text = df.iloc[i]['ArticleText']\n",
    "        splitted = text.split(sep='.')\n",
    "        for j in range(len(splitted)):\n",
    "            text = splitted[j]\n",
    "            if(text!=''):\n",
    "                words = text.split()\n",
    "                dataset.append(words)\n",
    "    dataset.append(df.iloc[i]['Question'].split())\n",
    "    dataset.append(df.iloc[i]['Answer'].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pz26dUeXmnCc"
   },
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(dataset, size=100, window=8, min_count=1, sg=0, workers=8) # I have 8 cpu cores\n",
    "# sg = {0, 1} â€“ Training algorithm: 1 for skip-gram; otherwise CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B7lXXAWImnCc",
    "outputId": "1b3b8bd5-1024-47a9-b228-d9aeb640ddbf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25700542, 32458150)"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(dataset, total_examples=len(dataset), compute_loss=True, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fEpFLCVcmnCd"
   },
   "outputs": [],
   "source": [
    "def get_embedding(sentence):\n",
    "  pos_sum = [0.0 for i in range(100)]\n",
    "  num = 0\n",
    "  words = sentence.split()\n",
    "  for i in words:\n",
    "    try:\n",
    "      embed = model.wv[i]\n",
    "    except:\n",
    "      continue\n",
    "    else:\n",
    "      pos_sum += embed\n",
    "      num +=1\n",
    "  if(num==0):\n",
    "    return pos_sum\n",
    "  else:\n",
    "    pos_sum /= num\n",
    "    return pos_sum\n",
    "\n",
    "def get_answer(question, answer_para):\n",
    "  question_embedding = get_embedding(rem_stop(question))\n",
    "  min_distance = math.inf\n",
    "  answer = 0\n",
    "  for i in range(len(answer_para)):\n",
    "    answer_embedding = get_embedding(rem_stop(answer_para[i]))\n",
    "    distance = np.linalg.norm(question_embedding-answer_embedding)\n",
    "    if (distance < min_distance):\n",
    "      answer = i\n",
    "      # print(answer)\n",
    "      min_distance = distance\n",
    "  return answer_para[answer]\n",
    "\n",
    "def rem_stop(sentence):\n",
    "    strr=''\n",
    "    my_string = sentence.split()\n",
    "    for i in range(len(my_string)):\n",
    "        if my_string[i] not in stopwords.words('english'):\n",
    "            strr = strr+' '+my_string[i]\n",
    "    return strr[1:]\n",
    "\n",
    "def get_answer_cosine(question, answer_para):\n",
    "  question_embedding = get_embedding(rem_stop(question))\n",
    "  max_similarity = -math.inf\n",
    "  answer = 0\n",
    "  for i in range(len(answer_para)):\n",
    "    answer_embedding = get_embedding(rem_stop(answer_para[i]))\n",
    "    similarity = cosine_similarity(np.expand_dims(question_embedding,0), np.expand_dims(answer_embedding,0))\n",
    "    if (similarity > max_similarity):\n",
    "      answer = i\n",
    "      max_similarity = similarity\n",
    "  return answer_para[answer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b7JkHV4UmnCd"
   },
   "outputs": [],
   "source": [
    "index = 296\n",
    "my_text = df.iloc[index]['ArticleText']\n",
    "temp_sentences = my_text.split(sep='.')\n",
    "sentences=[]\n",
    "for i in range(len(temp_sentences)):\n",
    "    if(temp_sentences[i]!=''):\n",
    "        sentences.append(temp_sentences[i])\n",
    "my_question = df.iloc[index]['Question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DBwIDuR3mnCd",
    "outputId": "ba017d02-5cb9-49c4-8594-49ac22b2331d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what was the consitution act formerly called\n",
      "consitution act formerly called\n",
      "british north america act\n"
     ]
    }
   ],
   "source": [
    "print(my_question) # Actual Question\n",
    "print(rem_stop(my_question)) # Answer without stopwords\n",
    "print(df.iloc[index]['Answer']) # Actual Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H4wJbW8smnCd",
    "outputId": "702ad9ea-1b27-4858-d6e8-4ded7bb5e83d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "3\n",
      "6\n",
      "7\n",
      "45\n",
      " to accommodate englishspeaking loyalists in quebec the constitutional act of 1791 divided the province into frenchspeaking lower canada and englishspeaking upper canada granting each their own elected legislative assembly\n",
      "\n",
      "\n",
      " later it was split into two british colonies called upper canada and lower canada until their union as the british province of canada in 1841\n"
     ]
    }
   ],
   "source": [
    "print(get_answer(my_question, sentences)) # Our model's prediction using euclidean distance\n",
    "print(\"\\n\")\n",
    "print(get_answer_cosine(my_question, sentences)) # Our model's prediction using cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IuYSwRVZmnCd"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "word2vec_baseline.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
